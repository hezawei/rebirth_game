"""
æ•…äº‹ç›¸å…³çš„APIç«¯ç‚¹
"""

from fastapi import APIRouter, HTTPException, status, Depends
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from typing import List, Dict, Optional, Tuple, Any
from collections import OrderedDict
# ã€æ ¸å¿ƒä¿®æ­£ã€‘å¯¼å…¥æ•´ä¸ªschemasæ¨¡å—ï¼Œè€Œä¸ä»…ä»…æ˜¯å…¶ä¸­çš„ç±»
from backend import schemas
from core.story_engine import story_engine
from core.history_context import build_prompt_context
from config.logging_config import LOGGER, story_logger, make_trace_id, kv_text, log_context
from config.settings import settings
from database.base import get_db, SessionLocal
from database import crud, models
# å¯¼å…¥æ–°çš„å®‰å…¨ä¾èµ–
from .user import get_current_user
from core.content_moderation import check_wish_safety_llm
from core.prompt_templates import PREPARE_LEVEL_PROMPT
from core.llm_clients import llm_client
from core.story_state import build_story_history, extract_chapter_number, build_story_segment_from_node
from core.speculation import speculation_service, speculation_get_metrics
import hashlib
import json
import re
import threading

SAVE_STATUSES = {"active", "completed", "failed"}


def _ensure_session_ownership(session: models.GameSession, user_id: str) -> None:
    if not session or session.user_id != user_id:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="æ— æƒè®¿é—®è¯¥æ¸¸æˆä¼šè¯")


def _ensure_node_belongs_to_session(node: models.StoryNode, session_id: int) -> None:
    if not node or node.session_id != session_id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠ‚ç‚¹ä¸å­˜åœ¨æˆ–ä¸å±äºè¯¥ä¼šè¯")


def _build_save_detail(db: Session, save: models.StorySave) -> schemas.story.StorySaveDetail:
    segment = build_story_segment_from_node(db, save.node, source="save")
    # é˜²æ­¢æ³„éœ²å†…éƒ¨å­—æ®µ
    try:
        segment = segment.model_copy(update={"metadata": _sanitize_metadata(segment.metadata)})
    except Exception:
        pass
    return schemas.story.StorySaveDetail(
        id=save.id,
        session_id=save.session_id,
        node_id=save.node_id,
        title=save.title,
        status=save.status,
        created_at=save.created_at,
        updated_at=save.updated_at,
        node=segment,
    )

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter()

# ç®€åŒ–çš„ç¬¬ä¸€èŠ‚æ•…äº‹é¢„ç”Ÿæˆç¼“å­˜
_FIRST_STORY_CACHE_MAX = getattr(settings, "first_story_cache_max_entries", 100)
_FIRST_STORY_CACHE: "OrderedDict[str, Dict[str, Any]]" = OrderedDict()  # key: f"{user_id}:{wish_digest}"
_CACHE_LOCK = threading.Lock()


def _make_cache_key(user_id: str, wish: str) -> str:
    wish_norm = (wish or "").strip()
    digest = hashlib.sha256(wish_norm.encode("utf-8")).hexdigest()
    return f"{user_id}:{digest}"


def _cache_store(key: str, value: Dict[str, Any]) -> None:
    with _CACHE_LOCK:
        if key in _FIRST_STORY_CACHE:
            _FIRST_STORY_CACHE.pop(key, None)
        elif len(_FIRST_STORY_CACHE) >= _FIRST_STORY_CACHE_MAX:
            evicted_key, _ = _FIRST_STORY_CACHE.popitem(last=False)
            LOGGER.info(f"[é¢„ç”Ÿæˆ] ğŸ“¦ ç¼“å­˜å·²æ»¡ï¼Œæ·˜æ±°æœ€æ—©çš„è®°å½•: {evicted_key}")
        _FIRST_STORY_CACHE[key] = value
        _FIRST_STORY_CACHE.move_to_end(key, last=True)


def _cache_pop(key: str) -> Optional[Dict[str, Any]]:
    with _CACHE_LOCK:
        value = _FIRST_STORY_CACHE.pop(key, None)
        if value:
            LOGGER.debug(f"[é¢„ç”Ÿæˆ] ä»ç¼“å­˜ä¸­å–å‡º key={key}")
        return value


def _cache_remove(key: str) -> None:
    with _CACHE_LOCK:
        if _FIRST_STORY_CACHE.pop(key, None) is not None:
            LOGGER.debug(f"[é¢„ç”Ÿæˆ] å·²ä»ç¼“å­˜ä¸­ç§»é™¤ key={key}")


def _background_generate_with_pregeneration(user_id: str, wish: str, trace: str | None = None) -> None:
    """åå°ç”Ÿæˆç¬¬ä¸€èŠ‚æ•…äº‹å¹¶åˆ›å»ºå®Œæ•´æ•°æ®åº“è®°å½•ï¼Œè§¦å‘é¢„ç”Ÿæˆ"""
    wish_norm = (wish or "").strip()
    log = story_logger(trace=trace or make_trace_id(), user_id=user_id, wish=wish_norm, task="pregeneration")
    db = SessionLocal()
    try:
        log.info("pregeneration start" + kv_text())

        session = crud.get_session_by_user_and_wish(db, user_id=user_id, wish=wish_norm)
        if session:
            log = log.bind(session=session.id)
            log.info("pregeneration reuse existing session")
        else:
            try:
                session = crud.create_game_session(db, wish=wish_norm, user_id=user_id)
                log = log.bind(session=session.id)
                log.info("pregeneration session created")
            except IntegrityError:
                db.rollback()
                session = crud.get_session_by_user_and_wish(db, user_id=user_id, wish=wish_norm)
                if not session:
                    raise
                log = log.bind(session=session.id)
                log.info("pregeneration session reused after IntegrityError")

        node = crud.get_root_node_for_session(db, session.id)
        if node:
            log = log.bind(node=node.id)
            log.info("pregeneration reuse root node")
        else:
            # 1. ç”Ÿæˆç¬¬ä¸€èŠ‚æ•…äº‹
            log.info("pregeneration story engine start")
            raw_data = story_engine.start_story(wish=wish_norm)
            log.info("pregeneration story ready" + kv_text(text_len=len(raw_data.text)))

            # 3. åˆ›å»ºç¬¬ä¸€ä¸ªæ•…äº‹èŠ‚ç‚¹
            node = crud.create_story_node(db, session_id=session.id, segment=raw_data)
            log = log.bind(node=node.id)
            log.info("pregeneration node created")
            log.info("pregeneration node image" + kv_text(image=raw_data.image_url))

        # 4. ç¼“å­˜ä¼šè¯å’ŒèŠ‚ç‚¹ä¿¡æ¯ä¾›startæ¥å£ä½¿ç”¨
        cache_key = _make_cache_key(user_id, wish_norm)
        _cache_store(cache_key, {
            "session_id": session.id,
            "node_id": node.id,
            "trace": trace,
        })
        log.info("pregeneration cache stored")

        # 5. è§¦å‘é¢„ç”Ÿæˆï¼šç¬¬ä¸€èŠ‚ç›¸å¯¹æ¦‚è¦å·²å 1å±‚ï¼Œè¿™é‡Œåªéœ€è¡¥é½åˆ° max_depthï¼Œå› æ­¤ä½¿ç”¨ (max_depth - 1)
        pre_depth = max(0, int(getattr(settings, 'speculation_max_depth', 0)) - 1)
        speculation_service.enqueue(session.id, node.id, depth=pre_depth)
        log.info("pregeneration speculation enqueued" + kv_text(depth=pre_depth))

        log.info("pregeneration done")
        
    except Exception as exc:
        log.error("pregeneration failed" + kv_text(error=str(exc)))
        db.rollback()
        # å¤±è´¥æ—¶æ¸…ç†ç¼“å­˜ï¼Œè®©startæ¥å£é™çº§åˆ°å®æ—¶ç”Ÿæˆ
        cache_key = _make_cache_key(user_id, wish_norm)
        _cache_remove(cache_key)
    finally:
        # æ— è®ºæˆåŠŸå¤±è´¥éƒ½ç¡®ä¿å…³é—­æ•°æ®åº“ä¼šè¯
        db.close()
        log.debug("pregeneration db closed")



def _wait_for_node_complete(node, db, max_wait_seconds: int = 60) -> bool:
    """
    ç­‰å¾…èŠ‚ç‚¹å®Œå…¨å‡†å¤‡å°±ç»ªï¼ˆæ•…äº‹+å›¾ç‰‡éƒ½å®Œæˆï¼‰
    åªæœ‰ç”¨æˆ·æ˜ç¡®é€‰æ‹©äº†è¯¥èŠ‚ç‚¹æ—¶æ‰ä¼šè°ƒç”¨æ­¤æ–¹æ³•
    """
    import time
    from pathlib import Path
    
    LOGGER.info(f"[NodeComplete] å¼€å§‹ç­‰å¾…èŠ‚ç‚¹å®Œå…¨å‡†å¤‡ï¼šnode_id={node.id}")
    
    start_time = time.time()
    check_interval = 0.5  # 500msæ£€æŸ¥ä¸€æ¬¡
    
    while time.time() - start_time < max_wait_seconds:
        # åˆ·æ–°èŠ‚ç‚¹çŠ¶æ€
        db.refresh(node)
        
        # æ£€æŸ¥1: æ•…äº‹æ–‡æœ¬æ˜¯å¦å­˜åœ¨
        if not node.story_text or len(node.story_text.strip()) == 0:
            LOGGER.debug(f"[NodeComplete] æ•…äº‹æ–‡æœ¬æœªå®Œæˆï¼Œç»§ç»­ç­‰å¾…ï¼šnode_id={node.id}")
            time.sleep(check_interval)
            continue
        
        # æ£€æŸ¥2: å›¾ç‰‡URLæ˜¯å¦å­˜åœ¨
        if not node.image_url:
            LOGGER.debug(f"[NodeComplete] å›¾ç‰‡URLæœªè®¾ç½®ï¼Œç»§ç»­ç­‰å¾…ï¼šnode_id={node.id}")
            time.sleep(check_interval)
            continue
        
        # æ£€æŸ¥3: å¦‚æœæ˜¯AIç”Ÿæˆçš„å›¾ç‰‡ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨ä¸”å¯è®¿é—®
        if node.image_url.startswith('/static/generated/'):
            # è¿™æ˜¯AIç”Ÿæˆçš„å›¾ç‰‡ï¼Œéœ€è¦éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
            filename = node.image_url.replace('/static/generated/', '')
            file_path = settings.BASE_DIR / "assets" / "generated_images" / filename
            
            if not file_path.exists():
                LOGGER.debug(f"[NodeComplete] AIç”Ÿæˆå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­ç­‰å¾…ï¼š{file_path}")
                time.sleep(check_interval)
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if file_path.stat().st_size == 0:
                LOGGER.debug(f"[NodeComplete] AIç”Ÿæˆå›¾ç‰‡æ–‡ä»¶ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…ï¼š{file_path}")
                time.sleep(check_interval)
                continue
            
            # å°è¯•è¯»å–æ–‡ä»¶å¤´ç¡®ä¿æ–‡ä»¶å®Œæ•´
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(10)
                    if len(header) == 0:
                        LOGGER.debug(f"[NodeComplete] AIç”Ÿæˆå›¾ç‰‡æ–‡ä»¶æ— æ³•è¯»å–ï¼Œç»§ç»­ç­‰å¾…ï¼š{file_path}")
                        time.sleep(check_interval)
                        continue
            except Exception as e:
                LOGGER.debug(f"[NodeComplete] AIç”Ÿæˆå›¾ç‰‡æ–‡ä»¶è®¿é—®å¼‚å¸¸ï¼Œç»§ç»­ç­‰å¾…ï¼š{e}")
                time.sleep(check_interval)
                continue
        
        # æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡
        elapsed = time.time() - start_time
        LOGGER.info(f"[NodeComplete] âœ… èŠ‚ç‚¹å®Œå…¨å‡†å¤‡å°±ç»ªï¼šnode_id={node.id} (ç­‰å¾…äº† {elapsed:.1f} ç§’)")
        return True
    
    # è¶…æ—¶
    elapsed = time.time() - start_time
    LOGGER.warning(f"[NodeComplete] â° èŠ‚ç‚¹å‡†å¤‡ç­‰å¾…è¶…æ—¶ï¼šnode_id={node.id} (ç­‰å¾…äº† {elapsed:.1f} ç§’)")
    return False


def _sanitize_metadata(meta: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """å»é™¤åªä¾›åç«¯ä½¿ç”¨çš„æ•æ„Ÿ/å†…éƒ¨å­—æ®µï¼Œé¿å…æ³„éœ²åˆ°å‰ç«¯ã€‚
    å½“å‰ä»…ç§»é™¤ chapter.hidden_effects_mapï¼Œä½†ä¿ç•™ chapter.enabled ç­‰å‰ç«¯éœ€è¦çš„æ ‡è¯†ã€‚
    """
    if not isinstance(meta, dict):
        return meta
    clean = dict(meta)
    chapter = clean.get("chapter")
    if isinstance(chapter, dict):
        chapter_clean = dict(chapter)
        # ç§»é™¤æ•æ„Ÿå­—æ®µ
        if "hidden_effects_map" in chapter_clean:
            chapter_clean.pop("hidden_effects_map", None)
        # ä¿ç•™å‰ç«¯éœ€è¦çš„æ ‡è¯†
        chapter_clean["hide_success_rate"] = True  # å‘Šè¯‰å‰ç«¯éšè—æˆåŠŸç‡æ˜¾ç¤º
        clean["chapter"] = chapter_clean
    return clean


@router.post("/check_wish", response_model=schemas.story.WishCheckResponse)
async def check_wish(
    request: schemas.story.WishCheckRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """ä½¿ç”¨LLMæ ¡éªŒç”¨æˆ·çš„é‡ç”Ÿæ„¿æœ›æ˜¯å¦è¿è§„"""
    text = (request.wish or "").strip()
    
    # åŸºæœ¬é•¿åº¦æ ¡éªŒ
    if not text or len(text) > 100:
        reason = "æ„¿æœ›ä¸èƒ½ä¸ºç©ºä¸”ä¸è¶…è¿‡100å­—"
        try:
            crud.log_wish_moderation(db, current_user.id, text, "rejected", reason)
        except Exception as exc:  # noqa: BLE001
            LOGGER.warning(f"[moderation] log wish failed: {exc}")
        return schemas.story.WishCheckResponse(ok=False, reason=reason)

    # ç›´æ¥ä½¿ç”¨LLMæ ¡éªŒ
    try:
        ok, reason = check_wish_safety_llm(text)
        status = "accepted" if ok else "rejected"
        
        try:
            crud.log_wish_moderation(db, current_user.id, text, status, reason)
        except Exception as exc:  # noqa: BLE001
            LOGGER.warning(f"[moderation] log wish failed: {exc}")
            
        LOGGER.info(f"[moderation] LLMæ ¡éªŒ user={current_user.id} ok={ok} reason={reason or '-'}")
        
        if not ok:
            return schemas.story.WishCheckResponse(ok=False, reason=reason or "æ„¿æœ›å†…å®¹ä¸åˆé€‚")
        
        return schemas.story.WishCheckResponse(ok=True)
        
    except Exception as exc:  # noqa: BLE001
        LOGGER.error(f"[moderation] LLMæ ¡éªŒå¤±è´¥ user={current_user.id}: {exc}")
        # æ ¡éªŒå¤±è´¥æ—¶ä¿å®ˆå¤„ç†
        try:
            crud.log_wish_moderation(db, current_user.id, text, "rejected", "ç³»ç»Ÿæ ¡éªŒå¤±è´¥")
        except Exception:
            pass
        return schemas.story.WishCheckResponse(ok=False, reason="æ„¿æœ›æ ¡éªŒå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")


@router.post("/prepare_start", response_model=schemas.story.PrepareStartResponse)
async def prepare_start_level(
    request: schemas.story.PrepareStartRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """
    æ ¹æ®æ„¿æœ›ç”Ÿæˆç¬¬ä¸€å…³çš„å…³å¡å…ƒä¿¡æ¯ï¼ˆä¸åˆ›å»ºä¼šè¯ï¼‰
    å‡è®¾æ„¿æœ›å·²é€šè¿‡æ ¡éªŒï¼Œç›´æ¥ç”Ÿæˆæ•…äº‹æ¦‚è¦ã€‚
    """
    # æ•…äº‹æ¦‚è¦ç”Ÿæˆï¼ˆè¿™æ˜¯è€—æ—¶æ“ä½œï¼Œåº”è¯¥åœ¨æ ¡éªŒä¹‹åå•ç‹¬è°ƒç”¨ï¼‰
    trace = make_trace_id()
    base_log = story_logger(
        trace=trace,
        user_id=getattr(current_user, "id", None),
        wish=request.wish.strip(),
        task="prepare_start",
    )
    base_log.info("PrepareStart start " + kv_text(wish=request.wish.strip()))

    base_log.debug("prepare context building")
    prompt_context = build_prompt_context(request.wish.strip())
    base_log.debug("prepare context ready " + kv_text(recommended_chapters=prompt_context.get("recommended_chapter_count")))
    
    tpl = PREPARE_LEVEL_PROMPT
    prompt = tpl.format(
        wish=request.wish.strip(),
        history_context=prompt_context["context_block"],
    )
    base_log.info("prepare LLM request " + kv_text(prompt_len=len(prompt)))
    raw = llm_client.generate(prompt)
    base_log.info("prepare LLM done " + kv_text(raw_len=len(str(raw))))
    try:
        s = str(raw).strip()
        # å…¼å®¹ ```json ä»£ç å›´æ 
        m = re.match(r"^\s*```(?:json)?\s*(.*?)\s*```\s*$", s, re.IGNORECASE | re.DOTALL)
        if m:
            s = m.group(1).strip()
        else:
            # å…¼å®¹å‰åå™ªå£°ï¼šä»é¦–ä¸ª '{' èµ·æŒ‰æ‹¬å·é…å¯¹æˆªå–
            start = s.find('{')
            if start != -1:
                depth = 0
                end = None
                for i in range(start, len(s)):
                    ch = s[i]
                    if ch == '{':
                        depth += 1
                    elif ch == '}':
                        depth -= 1
                        if depth == 0:
                            end = i
                            break
                if end is not None:
                    s = s[start:end+1].strip()
                else:
                    s = s[start:].strip()
        data = json.loads(s)
        base_log.info("prepare json parsed " + kv_text(fields="|".join(list(map(str, data.keys())))))
    except Exception as e:
        base_log.error("prepare json parse failed " + kv_text(error=str(e), preview=str(raw)[:120]))
        raise HTTPException(status_code=500, detail="ç”Ÿæˆå…³å¡å…ƒä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")

    level_title = data.get("level_title")
    background = data.get("background")
    # V2: ä» goal.description æ˜ å°„ main_quest
    main_quest = data.get("main_quest")
    if not main_quest and isinstance(data.get("goal"), dict):
        main_quest = data["goal"].get("description")
    
    base_log.debug("prepare data parsed " + kv_text(title=level_title, quest=main_quest))
    
    if not all([level_title, background, main_quest]):
        base_log.error("prepare missing fields" + kv_text(payload=str(data)))
        raise HTTPException(status_code=500, detail="ç”Ÿæˆå…³å¡å…ƒä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·ç¨åé‡è¯•")

    meta = {
        "wish": request.wish.strip(),
        "history_profile": prompt_context["profile_dict"],
        "recommended_chapter_count": prompt_context["recommended_chapter_count"],
        "anchor_events": prompt_context["anchor_events"],
    }
    # è¿”å›é˜ˆå€¼ä»¥ä¾¿å‰ç«¯å±•ç¤ºç›®æ ‡è¯´æ˜ï¼ˆä¸æ•æ„Ÿï¼‰
    meta.update({
        "prepare": {
            "goal": data.get("goal"),
            "min_nodes": data.get("min_nodes"),
            "max_nodes": data.get("max_nodes"),
            "pass_threshold": data.get("pass_threshold"),
            "fail_threshold": data.get("fail_threshold"),
        }
        })

    # è§¦å‘åå°å®Œæ•´æ•…äº‹ç”Ÿæˆæµç¨‹ï¼ˆåŒ…å«sessionåˆ›å»ºå’Œspeculationé¢„ç”Ÿæˆï¼‰
    wish_norm = request.wish.strip()
    try:
        thread = threading.Thread(
            target=_background_generate_with_pregeneration,
            args=(str(current_user.id), wish_norm, trace),
            daemon=True
        )
        thread.start()
        base_log.info("prepare background thread started" + kv_text(user=current_user.id))
    except Exception as exc:
        base_log.warning("prepare background thread failed " + kv_text(error=str(exc)))

    return schemas.story.PrepareStartResponse(
        level_title=level_title,
        background=background,
        main_quest=main_quest,
        metadata=meta
    )


@router.post("/start", response_model=schemas.story.StorySegment)
async def start_new_story(
    request: schemas.story.StoryStartRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """
    å¼€å§‹æ–°çš„é‡ç”Ÿæ•…äº‹ (éœ€è¦è®¤è¯)

    Args:
        request: åŒ…å«ç”¨æˆ·é‡ç”Ÿæ„¿æœ›çš„è¯·æ±‚
        db: æ•°æ®åº“ä¼šè¯
        current_user: ä»JWTè·å–çš„å½“å‰ç”¨æˆ·æ¨¡å‹

    Returns:
        StorySegment: åŒ…å«æ•…äº‹æ–‡æœ¬ã€é€‰æ‹©é€‰é¡¹å’Œå›¾ç‰‡çš„å“åº”
    """

    user_id = current_user.id
    wish_norm = request.wish.strip()
    trace = make_trace_id()
    base_log = story_logger(
        trace=trace,
        user_id=user_id,
        wish=wish_norm,
        task="start",
    )
    base_log.info("start request " + kv_text(wish=wish_norm))

    # åŸºæœ¬éªŒè¯ï¼ˆæ„¿æœ›åº”è¯¥å·²é€šè¿‡check_wishæ ¡éªŒï¼‰
    base_log.debug("start wish validated")

    # ä¼˜å…ˆä½¿ç”¨é¢„ç”Ÿæˆçš„sessionå’Œnodeï¼Œå¦åˆ™é™çº§åˆ°å®æ—¶ç”Ÿæˆ
    cache_key = _make_cache_key(user_id, wish_norm)
    base_log.debug("start cache key" + kv_text(cache_key=cache_key))
    
    cached_data = None
    cache_wait_seconds = getattr(settings, "start_cache_wait_seconds", 8)
    poll_interval = 0.4
    elapsed = 0.0

    cached_data = _cache_pop(cache_key)
    if cached_data and cached_data.get("trace"):
        trace = cached_data["trace"]
        base_log = story_logger(
            trace=trace,
            user_id=user_id,
            wish=wish_norm,
            task="start",
        )
        base_log.info("start trace resumed")
    base_log.info("start cache first hit" + kv_text(hit=bool(cached_data)))

    while cached_data is None and elapsed < cache_wait_seconds:
        remaining = cache_wait_seconds - elapsed
        wait = poll_interval if remaining > poll_interval else remaining
        base_log.info("start cache wait" + kv_text(wait=wait, elapsed=elapsed, limit=cache_wait_seconds))
        threading.Event().wait(wait)
        elapsed += wait
        cached_data = _cache_pop(cache_key)
        if cached_data:
            base_log.info("start cache success" + kv_text(session_id=cached_data["session_id"], node_id=cached_data["node_id"], elapsed=elapsed))

    if cached_data is not None:
        # ä½¿ç”¨é¢„ç”Ÿæˆçš„sessionå’Œnode
        session_id = cached_data["session_id"]
        node_id = cached_data["node_id"]
        start_log = base_log.bind(session=session_id, node=node_id)
        start_log.info("start use pregenerated")
        
        session = crud.get_session_by_id(db, session_id)
        node = crud.get_node_by_id(db, node_id)
        
        if not session or not node:
            start_log.warning("start cache miss objects" + kv_text(session_exists=bool(session), node_exists=bool(node)))
            # é™çº§åˆ°å®æ—¶ç”Ÿæˆ
            start_log.info("start fallback create session")
            session = crud.create_game_session(db, wish=wish_norm, user_id=user_id)
            start_log.info("start fallback session created" + kv_text(session_id=session.id))
            
            start_log.info("start fallback generate story")
            raw_data = story_engine.start_story(wish=wish_norm)
            start_log.info("start fallback story done" + kv_text(text_len=len(raw_data.text)))
            
            start_log.info("start fallback save node")
            node = crud.create_story_node(db, session_id=session.id, segment=raw_data)
            start_log.info("start fallback node saved" + kv_text(node_id=node.id))
        else:
            # ä½¿ç”¨é¢„ç”Ÿæˆçš„èŠ‚ç‚¹ï¼Œæ— éœ€raw_data
            start_log.info("start use cached node" + kv_text(session_id=session.id, node_id=node.id))
            raw_data = None
    else:
        start_log = base_log
        start_log.info("start cache miss -> realtime")
        # é™çº§åˆ°å®æ—¶ç”Ÿæˆ
        start_log.info("start realtime create session")
        session = crud.create_game_session(db, wish=wish_norm, user_id=user_id)
        start_log = start_log.bind(session=session.id)
        start_log.info("start realtime session created" + kv_text(session_id=session.id))

        start_log.info("start realtime generate story")
        raw_data = story_engine.start_story(wish=wish_norm)
        start_log.info("start realtime story done" + kv_text(text_len=len(raw_data.text)))

        start_log.info("start realtime save node")
        node = crud.create_story_node(db, session_id=session.id, segment=raw_data)
        start_log = start_log.bind(node=node.id)
        start_log.info("start realtime node saved" + kv_text(node_id=node.id))

    # 4. ã€é€»è¾‘ç®€åŒ–ã€‘å¼€å§‹æ•…äº‹æ°¸è¿œæ˜¯ç¬¬1ç« 
    chapter_number = 1

    # 5. ç»„åˆæœ€ç»ˆå“åº”
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç¡®ä¿ metadata ç»“æ„ä¸€è‡´
    if raw_data is not None:
        # å®æ—¶ç”Ÿæˆçš„æƒ…å†µï¼Œä½¿ç”¨raw_data
        metadata = {
            **(raw_data.metadata or {}),
            "chapter_number": chapter_number
        }
        choices_payload = [
            c.model_dump() if hasattr(c, "model_dump") else c
            for c in (raw_data.choices or [])
        ]
        story_text = raw_data.text
        image_url = raw_data.image_url
        success_rate = raw_data.success_rate
    else:
        # ä½¿ç”¨é¢„ç”ŸæˆèŠ‚ç‚¹çš„æƒ…å†µï¼Œä»nodeè·å–æ•°æ®
        metadata = {
            **(node.get_metadata() or {}),
            "chapter_number": chapter_number
        }
        choices_payload = node.get_choices() or []
        story_text = node.story_text  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§å
        image_url = node.image_url
        success_rate = node.success_rate
    
    result = schemas.story.StorySegment(
        session_id=session.id,
        node_id=node.id,
        text=story_text,
        choices=choices_payload,
        image_url=image_url,
        success_rate=success_rate,
        metadata=_sanitize_metadata(metadata),
    )

    start_log = start_log.bind(session=result.session_id, node=result.node_id)
    start_log.info("start done" + kv_text(text_len=len(result.text), choice_count=len(result.choices or []), image=result.image_url))
    # ã€è°ƒè¯•ã€‘æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨
    if result.image_url and '/static/generated/' in result.image_url:
        filename = result.image_url.split('/')[-1]
        local_file_path = settings.BASE_DIR / "assets" / "generated_images" / filename
        file_exists = local_file_path.exists()
        file_size = local_file_path.stat().st_size if file_exists else 0
        start_log.info("start image file" + kv_text(file_exists=file_exists, size=file_size, path=local_file_path))

    # åŠ¨æ€çª—å£ï¼šæ— è®ºæ˜¯å¦å‘½ä¸­é¢„ç”Ÿæˆï¼Œéƒ½è¦ä»â€œå½“å‰èŠ‚ç‚¹=ç¬¬ä¸€èŠ‚â€è¡¥é½åˆ° max_depth å±‚
    start_log.info("start speculation enqueue" + kv_text(depth=settings.speculation_max_depth))
    with log_context(trace=trace, user=user_id, session=session.id, node=node.id, task="speculation"):
        speculation_service.enqueue(session.id, node.id, depth=settings.speculation_max_depth)
    start_log.info("start response ready" + kv_text(image=result.image_url))
    return result


@router.post("/continue", response_model=schemas.story.StorySegment)
async def continue_existing_story(
    request: schemas.story.StoryContinueRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """
    ç»§ç»­ç°æœ‰æ•…äº‹

    Args:
        request: åŒ…å«session_idã€node_idå’Œç”¨æˆ·é€‰æ‹©çš„è¯·æ±‚
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        StorySegment: åŒ…å«æ–°æ•…äº‹æ–‡æœ¬ã€é€‰æ‹©é€‰é¡¹å’Œå›¾ç‰‡çš„å“åº”
    """
    trace = make_trace_id()
    base_log = story_logger(
        trace=trace,
        user_id=current_user.id,
        session_id=request.session_id,
        node_id=request.node_id,
        task="continue",
    )
    base_log.info("continue request" + kv_text(choice=request.choice))

    # éªŒè¯è¾“å…¥
    if not request.choice or len(request.choice.strip()) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ç”¨æˆ·é€‰æ‹©ä¸èƒ½ä¸ºç©º"
        )

    # 0. æƒé™ä¸å½’å±æ ¡éªŒ
    session = crud.get_session_by_id(db, request.session_id)
    if not session or session.user_id != current_user.id:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="æ— æƒè®¿é—®è¯¥æ¸¸æˆä¼šè¯")

    # æ ¡éªŒèŠ‚ç‚¹æ˜¯å¦å±äºè¯¥ä¼šè¯
    parent_node = crud.get_node_by_id(db, request.node_id)
    if not parent_node or parent_node.session_id != request.session_id:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠ‚ç‚¹ä¸å­˜åœ¨æˆ–ä¸å±äºè¯¥ä¼šè¯")

    base_log = base_log.bind(session=session.id, node=parent_node.id)

    # 1. æ„å»ºä»æ ¹åˆ°å½“å‰çˆ¶èŠ‚ç‚¹çš„è·¯å¾„å†å²
    story_history = build_story_history(db, parent_node)
    parent_chapter = extract_chapter_number(parent_node)
    current_success_rate = parent_node.success_rate
    
    # success_rateå¯èƒ½ä¸ºNoneï¼ˆéšè—æ•°å€¼ï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸çš„

    # ç«æ€ä¿æŠ¤ï¼šå¦‚æœè¯¥é€‰é¡¹æ­£åœ¨ç”Ÿæˆä¸­ï¼Œç­‰å¾…å…¶å®Œæˆï¼ˆå¯¹ç”¨æˆ·æ— æ„Ÿï¼‰
    wait_interval = 0.3
    while speculation_service.is_choice_generating(request.session_id, request.node_id, request.choice.strip()):
        import time
        time.sleep(wait_interval)

    # åŠ¨æ€çª—å£ï¼šä¸åšè¿‡æœŸæ¸…ç†ï¼Œä¿ç•™å¯å¤ç”¨çš„é¢„æ¨æ¼”ç¼“å­˜

    # 2. å¹‚ç­‰æ€§æ£€æŸ¥ï¼šæ˜¯å¦å·²ç»å­˜åœ¨åŒçˆ¶èŠ‚ç‚¹ã€åŒé€‰æ‹©çš„å­èŠ‚ç‚¹ï¼ˆå¤„ç†åŒå‡»/å¹¶å‘é‡æ”¾ï¼‰
    existing_child = crud.get_child_by_parent_and_choice(
        db, request.session_id, request.node_id, request.choice.strip()
    )
    if existing_child:
        if existing_child.is_speculative:
            existing_child = crud.finalize_speculative_node(db, existing_child)
        
        # ã€èŠ‚ç‚¹å®Œæ•´æ€§æ£€æŸ¥ã€‘ç¡®ä¿ç”¨æˆ·é€‰æ‹©çš„èŠ‚ç‚¹(æ•…äº‹+å›¾ç‰‡)éƒ½å®Œå…¨å‡†å¤‡å¥½äº†
        child_log = base_log.bind(node=existing_child.id)
        child_log.info("continue node ready check")

        if _wait_for_node_complete(existing_child, db):
            child_log.info("continue node ready success")
        else:
            child_log.warning("continue node ready timeout")

        chapter_number = crud.calculate_chapter_number(db, request.session_id, existing_child.id)
        metadata = existing_child.get_metadata() or {}
        metadata.update({"source": "continue", "chapter_number": chapter_number})
        metadata = _sanitize_metadata(metadata)
        # è¡¥é½ä»¥â€œå½“å‰èŠ‚ç‚¹=å·²é€‰æ‹©çš„å­èŠ‚ç‚¹â€ä¸ºé”šçš„ max_depth çª—å£
        child_log.info("continue speculation enqueue existing" + kv_text(depth=settings.speculation_max_depth))
        with log_context(trace=trace, user=current_user.id, session=request.session_id, node=existing_child.id, task="speculation"):
            speculation_service.enqueue(request.session_id, existing_child.id, depth=settings.speculation_max_depth)
        return schemas.story.StorySegment(
            session_id=request.session_id,
            node_id=existing_child.id,
            text=existing_child.story_text,
            choices=existing_child.get_choices(),
            image_url=existing_child.image_url,
            success_rate=existing_child.success_rate,
            metadata=metadata,
        )

    # 2b. è°ƒç”¨å¼•æ“ç”Ÿæˆä¸‹ä¸€æ®µæ•…äº‹ (è¿”å›RawStoryData) â€” åœ¨äº‹åŠ¡ä¹‹å¤–æ‰§è¡Œï¼Œé¿å…é•¿äº‹åŠ¡
    base_log.info("continue generate child" + kv_text(choice=request.choice.strip()))
    raw_data = story_engine.continue_story(
        wish=session.wish,
        story_history=story_history,
        choice=request.choice.strip(),
        chapter_number=parent_chapter,
        current_success_rate=current_success_rate,
        parent_metadata=parent_node.get_metadata(),
    )

    # 3. åˆ›å»ºæ–°çš„æ•…äº‹èŠ‚ç‚¹ï¼ˆçŸ­äº‹åŠ¡å†…æ‰§è¡Œï¼šåŠ é” -> äºŒæ¬¡æ£€æŸ¥ -> æ’å…¥/flush -> æäº¤ï¼‰
    try:
        # ä½¿ç”¨ä¼šè¯çš„è‡ªåŠ¨äº‹åŠ¡ï¼Œé¿å…æ˜¾å¼ begin() å¯¼è‡´â€œå·²å­˜åœ¨äº‹åŠ¡â€çš„é”™è¯¯
        # 3a. åœ¨æ”¯æŒçš„æ•°æ®åº“ä¸Šé”å®šçˆ¶èŠ‚ç‚¹ï¼Œç¼©çŸ­é”æ—¶é—´çª—å£
        _ = crud.lock_node_for_update(db, request.node_id)
        # 3b. å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²è¢«å¹¶å‘è¯·æ±‚åˆ›å»ºï¼ˆåŒé‡æ ¡éªŒï¼‰
        concurrent_child = crud.get_child_by_parent_and_choice(
            db, request.session_id, request.node_id, request.choice.strip()
        )
        if concurrent_child:
            new_node = concurrent_child
        else:
            new_node = crud.create_story_node(
                db,
                session_id=request.session_id,
                segment=raw_data,
                parent_id=request.node_id,
                user_choice=request.choice.strip(),
                commit=False,
            )
        # æ˜ç¡®æäº¤äº‹åŠ¡
        db.commit()
    except IntegrityError:
        # å¯èƒ½åœ¨æäº¤æ—¶è§¦å‘å”¯ä¸€çº¦æŸï¼Œè¯´æ˜å¹¶å‘è¯·æ±‚å·²åˆ›å»ºç›¸åŒå­èŠ‚ç‚¹
        db.rollback()
        new_node = crud.get_child_by_parent_and_choice(
            db, request.session_id, request.node_id, request.choice.strip()
        )
        if not new_node:
            raise

    new_log = base_log.bind(node=new_node.id)
    new_log.info("continue node created" + kv_text(parent=request.node_id))

    # 4. ã€æ ¸å¿ƒä¿®æ”¹ã€‘åœ¨è¿™é‡Œè°ƒç”¨ä¸€æ¬¡ calculate_chapter_number å³å¯
    chapter_number = crud.calculate_chapter_number(db, request.session_id, new_node.id)

    # 5. ç»„åˆå“åº”
    metadata = {
        **(raw_data.metadata or {}),
        "chapter_number": chapter_number
    }
    # å°† ChoiceOption å®ä¾‹è½¬æ¢ä¸ºå­—å…¸ï¼Œé¿å… Pydantic ç±»å‹èº«ä»½ä¸ä¸€è‡´å¯¼è‡´çš„æ ¡éªŒé”™è¯¯
    choices_payload = [
        c.model_dump() if hasattr(c, "model_dump") else c
        for c in (raw_data.choices or [])
    ]
    result = schemas.story.StorySegment(
        session_id=request.session_id,
        node_id=new_node.id,
        text=raw_data.text,
        choices=choices_payload,
        image_url=raw_data.image_url,
        success_rate=raw_data.success_rate,
        metadata=_sanitize_metadata(metadata)
    )

    new_log.info("continue response ready" + kv_text(text_len=len(raw_data.text), choices=len(choices_payload)))
    new_log.info("continue speculation enqueue new" + kv_text(depth=settings.speculation_max_depth))
    with log_context(trace=trace, user=current_user.id, session=request.session_id, node=new_node.id, task="speculation"):
        speculation_service.enqueue(request.session_id, new_node.id, depth=settings.speculation_max_depth)
    return result


# ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¿®æ”¹ /retry ç«¯ç‚¹çš„å‡½æ•°ç­¾åå’Œè¯·æ±‚å¤„ç†
@router.post("/retry", response_model=schemas.story.StorySegment)
async def retry_from_node(
    request: schemas.story.StoryRetryRequest,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """
    ä»æŸä¸ªæ•…äº‹èŠ‚ç‚¹é‡æ–°å¼€å§‹

    Args:
        request: åŒ…å« "node_id" çš„è¯·æ±‚ï¼Œnode_idæ˜¯ç©å®¶åæ‚”çš„é‚£ä¸ªé€‰æ‹©æ‰€äº§ç”Ÿçš„èŠ‚ç‚¹ID

    Returns:
        StorySegment: ç©å®¶å¯ä»¥é‡æ–°å¼€å§‹çš„é‚£ä¸ªçˆ¶èŠ‚ç‚¹çš„å®Œæ•´ä¿¡æ¯
    """
    node_id = request.node_id  # ç›´æ¥ä»éªŒè¯è¿‡çš„æ¨¡å‹ä¸­è·å–
    LOGGER.info(f"æ”¶åˆ°æ—¶ç©ºå›æº¯è¯·æ±‚ï¼Œç›®æ ‡èŠ‚ç‚¹: {node_id}")

    try:
        # æƒé™ä¸å½’å±æ ¡éªŒ
        original_node = crud.get_node_by_id(db, node_id)
        if not original_node:
            raise ValueError("èŠ‚ç‚¹ä¸å­˜åœ¨")
        session = crud.get_session_by_id(db, original_node.session_id)
        if not session or session.user_id != current_user.id:
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="æ— æƒæ“ä½œè¯¥æ¸¸æˆä¼šè¯")

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è°ƒç”¨æ–°çš„ CRUD å‡½æ•°
        target_node = crud.prune_story_after_node(db, node_id)

        # è®¡ç®—å›æº¯ç‚¹çš„ç« èŠ‚å·
        chapter_number = crud.calculate_chapter_number(db, target_node.session_id, target_node.id)

        # å°†è¿”å›çš„ SQLAlchemy æ¨¡å‹å¯¹è±¡è½¬æ¢ä¸º Pydantic æ¨¡å‹
        result = schemas.story.StorySegment(
            session_id=target_node.session_id,
            node_id=target_node.id,
            text=target_node.story_text,
            choices=target_node.get_choices(),
            image_url=target_node.image_url,
            success_rate=target_node.success_rate,
            metadata={"source": "retry", "chapter_number": chapter_number}
        )
        return result

    except ValueError as e:
        LOGGER.error(f"æ—¶ç©ºå›æº¯å¤±è´¥: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        LOGGER.error(f"å¤„ç†å›æº¯è¯·æ±‚æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æ—¶ç©ºå›æº¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯")


# --- é‡ç”Ÿç¼–å¹´å² (Chronicle) API ---

@router.get("/sessions/{session_id}/latest", response_model=schemas.story.StorySegment)
async def get_latest_node_for_session(
    session_id: int,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """è·å–æŒ‡å®šä¼šè¯çš„æœ€æ–°èŠ‚ç‚¹ä»¥ç»§ç»­æ¸¸æˆ (éœ€è¦è®¤è¯)"""
    LOGGER.info(f"[latest] user={current_user.id} querying session_id={session_id}")
    node = crud.get_latest_node_by_session(db, session_id=session_id, user_id=current_user.id)
    
    if not node:
        LOGGER.warning(f"[latest] no node found for session_id={session_id} or not owned by user")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°è¯¥æ¸¸æˆä¼šè¯çš„æœ€æ–°èŠ‚ç‚¹ï¼Œæˆ–è¯¥ä¼šè¯ä¸å±äºä½ ")

    chapter_number = crud.calculate_chapter_number(db, node.session_id, node.id)
    
    metadata = {
        "source": "continue",
        "chapter_number": chapter_number
    }
    metadata = _sanitize_metadata(metadata)
    segment = schemas.story.StorySegment(
        session_id=node.session_id,
        node_id=node.id,
        text=node.story_text,
        choices=node.get_choices(),
        image_url=node.image_url,
        success_rate=node.success_rate,
        metadata=metadata
    )
    LOGGER.info(f"[latest] returning node_id={segment.node_id} session_id={segment.session_id} chapter={chapter_number} choices={len(segment.choices) if segment.choices else 0}")
    return segment

@router.get("/sessions", response_model=List[schemas.story.GameSessionSummary])
async def get_user_sessions(
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """è·å–å½“å‰ç™»å½•ç”¨æˆ·çš„æ‰€æœ‰æ¸¸æˆä¼šè¯åˆ—è¡¨ (éœ€è¦è®¤è¯)"""
    sessions = crud.get_sessions_by_user(db, user_id=current_user.id)
    return sessions

@router.get("/latest", response_model=schemas.story.StorySegment)
async def get_user_latest_node(
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """è·å–å½“å‰ç”¨æˆ·æ‰€æœ‰ä¼šè¯ä¸­çš„æœ€æ–°èŠ‚ç‚¹ï¼ˆç”¨äºæœ€å¯ä¿¡çš„ç»§ç»­æ¸¸æˆï¼‰"""
    LOGGER.info(f"[latest-global] user={current_user.id} querying deepest progress across all sessions")
    node = crud.get_deepest_node_for_user(db, user_id=current_user.id)
    if not node:
        LOGGER.warning(f"[latest-global] user={current_user.id} has no nodes across any sessions")
        raise HTTPException(status_code=404, detail="ä½ è¿˜æ²¡æœ‰ä»»ä½•æ¸¸æˆè¿›åº¦")

    chapter_number = crud.calculate_chapter_number(db, node.session_id, node.id)
    metadata = {
        "source": "continue",
        "chapter_number": chapter_number
    }
    metadata = _sanitize_metadata(metadata)
    segment = schemas.story.StorySegment(
        session_id=node.session_id,
        node_id=node.id,
        text=node.story_text,
        choices=node.get_choices(),
        image_url=node.image_url,
        metadata=metadata
    )
    LOGGER.info(f"[latest-global] returning node_id={segment.node_id} session_id={segment.session_id} chapter={chapter_number} choices={len(segment.choices) if segment.choices else 0}")
    return segment

@router.get("/sessions/{session_id}", response_model=schemas.story.GameSessionDetail)
async def get_session_details(
    session_id: int,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    """è·å–å•ä¸ªæ¸¸æˆä¼šè¯çš„è¯¦ç»†å†å² (éœ€è¦è®¤è¯)"""
    session = crud.get_session_details(db, session_id=session_id, user_id=current_user.id)
    if not session:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°è¯¥æ¸¸æˆä¼šè¯ï¼Œæˆ–è¯¥ä¼šè¯ä¸å±äºä½ ")

    # ä»…æå–å·²ç¡®è®¤ï¼ˆéé¢„ç”Ÿæˆï¼‰çš„èŠ‚ç‚¹ï¼Œé¿å…æå‰å‰§é€
    confirmed_nodes = crud.get_session_history(db, session.id)

    # æ‰‹åŠ¨æ„å»ºå“åº”æ¨¡å‹ï¼Œå› ä¸ºæ•°æ®åº“æ¨¡å‹å’ŒPydanticæ¨¡å‹ç»“æ„ä¸å®Œå…¨åŒ¹é…
    nodes_details = []
    for i, node in enumerate(confirmed_nodes):
        nodes_details.append(schemas.story.StoryNodeDetail(
            id=node.id,
            story_text=node.story_text,
            image_url=node.image_url,
            user_choice=node.user_choice,
            choices=node.get_choices(),
            chapter_number=i + 1
        ))

    return schemas.story.GameSessionDetail(
        id=session.id,
        wish=session.wish,
        created_at=session.created_at,
        nodes=nodes_details
    )


@router.post("/saves", response_model=schemas.story.StorySaveDetail)
async def create_story_save_endpoint(
    request: schemas.story.StorySaveCreate,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    session = crud.get_session_by_id(db, request.session_id)
    _ensure_session_ownership(session, current_user.id)

    node = crud.get_node_by_id(db, request.node_id)
    _ensure_node_belongs_to_session(node, request.session_id)

    save = crud.create_story_save(
        db,
        session_id=request.session_id,
        node_id=request.node_id,
        title=request.title,
    )
    return _build_save_detail(db, save)


@router.get("/saves", response_model=List[schemas.story.StorySaveSummary])
async def list_story_saves_endpoint(
    status_filter: Optional[str] = None,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    status_value = status_filter.strip() if status_filter else None
    if status_value and status_value not in SAVE_STATUSES:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="éæ³•çš„å­˜æ¡£çŠ¶æ€è¿‡æ»¤")

    saves = crud.list_story_saves(db, current_user.id, status=status_value)
    return [
        schemas.story.StorySaveSummary(
            id=save.id,
            session_id=save.session_id,
            node_id=save.node_id,
            title=save.title,
            status=save.status,
            created_at=save.created_at,
            updated_at=save.updated_at,
        )
        for save in saves
    ]


@router.get("/saves/{save_id}", response_model=schemas.story.StorySaveDetail)
async def get_story_save_detail_endpoint(
    save_id: int,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    save = crud.get_story_save(db, save_id, current_user.id)
    if not save:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="æ‰¾ä¸åˆ°å­˜æ¡£")
    return _build_save_detail(db, save)


@router.patch("/saves/{save_id}", response_model=schemas.story.StorySaveDetail)
async def update_story_save_endpoint(
    save_id: int,
    request: schemas.story.StorySaveUpdate,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    save = crud.get_story_save(db, save_id, current_user.id)
    if not save:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="æ‰¾ä¸åˆ°å­˜æ¡£")

    new_status: Optional[str] = request.status
    if new_status and new_status not in SAVE_STATUSES:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="éæ³•çš„å­˜æ¡£çŠ¶æ€")

    updated = crud.update_story_save(db, save_id, title=request.title, status=new_status)
    if not updated:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="å­˜æ¡£æ›´æ–°å¤±è´¥")

    return _build_save_detail(db, updated)


@router.delete("/saves/{save_id}", response_model=schemas.story.StorySaveSummary)
async def delete_story_save_endpoint(
    save_id: int,
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    save = crud.get_story_save(db, save_id, current_user.id)
    if not save:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="æ‰¾ä¸åˆ°å­˜æ¡£")

    summary = schemas.story.StorySaveSummary(
        id=save.id,
        session_id=save.session_id,
        node_id=save.node_id,
        title=save.title,
        status=save.status,
        created_at=save.created_at,
        updated_at=save.updated_at,
    )

    if not crud.delete_story_save(db, save_id, current_user.id):
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="åˆ é™¤å­˜æ¡£å¤±è´¥")

    return summary



@router.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    """
    return {"status": "healthy", "service": "story_api"}


@router.get("/metrics")
async def get_metrics():
    """è¿”å›æœåŠ¡ç«¯å…³é”®è¿è¡ŒæŒ‡æ ‡ï¼ˆLLM ä¸ æ¨æ¼”æœåŠ¡ï¼‰ã€‚"""
    try:
        from core.llm_clients import llm_client
        llm_metrics = getattr(llm_client, "get_metrics", lambda: {} )()
    except Exception as e:  # noqa: BLE001
        llm_metrics = {"error": str(e)}

    try:
        spec_metrics = speculation_get_metrics()
    except Exception as e:  # noqa: BLE001
        spec_metrics = {"error": str(e)}

    return {
        "llm": llm_metrics,
        "speculation": spec_metrics,
    }


# æ³¨æ„ï¼šå¼‚å¸¸å¤„ç†å™¨åº”è¯¥åœ¨ä¸»åº”ç”¨ä¸­å®šä¹‰ï¼Œä¸æ˜¯åœ¨è·¯ç”±å™¨ä¸­
